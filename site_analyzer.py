# site_analyzer.py
import requests
from bs4 import BeautifulSoup
import whois
from datetime import datetime, timedelta
import re
from urllib.parse import urlparse
import logging
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SiteAnalyzer:
    def __init__(self):
        self.results = {}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def analyze_site(self, url):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞"""
        try:
            if not self.is_valid_url(url):
                return "‚ùå –û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL"

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url

            domain = self.extract_domain(url)
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞: {domain}")

            # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.results.clear()

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏
            self.check_domain_age(domain)

            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞
            try:
                response = requests.get(url, headers=self.headers, timeout=15, verify=True)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                self.check_content_updates(soup, response)
                self.check_page_structure(soup)
                self.check_builder(soup, url, response)
            except requests.RequestException as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∞–π—Ç–∞: {e}")
                self.results['–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å'] = 'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (—Å–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)'

            self.check_owner(domain)
            self.check_reviews(domain)

            return self.generate_report()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–∞–π—Ç–∞: {str(e)}"

    def is_valid_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        pattern = re.compile(
            r'^(https?://)?'  # –ø—Ä–æ—Ç–æ–∫–æ–ª
            r'((([a-z\d]([a-z\d-]*[a-z\d])*)\.)+[a-z]{2,}|'  # –¥–æ–º–µ–Ω
            r'((\d{1,3}\.){3}\d{1,3}))'  # –∏–ª–∏ IP
            r'(:\d+)?'  # –ø–æ—Ä—Ç
            r'(/[-a-z\d%_.~+]*)*'  # –ø—É—Ç—å
            r'(\?[;&a-z\d%_.~+=-]*)?'  # query string
            r'(#[-a-z\d_]*)?$', re.IGNORECASE)
        return pattern.match(url) is not None

    def extract_domain(self, url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        parsed = urlparse(url)
        return parsed.netloc or parsed.path

    def check_domain_age(self, domain):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ –¥–æ–º–µ–Ω–∞"""
        try:
            w = whois.whois(domain)
            creation_date = w.creation_date

            if isinstance(creation_date, list):
                creation_date = creation_date[0]

            if creation_date:
                age = datetime.now() - creation_date
                days = age.days
                months = days // 30

                if days < 120:  # 4 –º–µ—Å—è—Ü–∞
                    self.results['–í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞'] = f'üî¥ –ù–µ–≥–∞—Ç–∏–≤ ({months} –º–µ—Å.)'
                else:
                    self.results['–í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞'] = f'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ ({months} –º–µ—Å.)'
            else:
                self.results['–í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ WHOIS: {e}")
            self.results['–í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def check_content_updates(self, soup, response):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            current_year = datetime.now().year
            current_month = datetime.now().month

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –≤ headers
            if 'last-modified' in response.headers:
                last_modified = response.headers['last-modified']
                self.results['–û–±–Ω–æ–≤–ª–µ–Ω–∏—è'] = f'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ (–ø–æ—Å–ª–µ–¥–Ω–µ–µ: {last_modified[:20]})'
                return

            # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            text = soup.get_text()
            date_patterns = [
                r'\b\d{2}[./-]\d{2}[./-]\d{4}\b',  # DD.MM.YYYY
                r'\b\d{4}[./-]\d{2}[./-]\d{2}\b',  # YYYY-MM-DD
                r'\b(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4}\b',
            ]

            found_dates = []
            for pattern in date_patterns:
                dates = re.findall(pattern, text, re.IGNORECASE)
                found_dates.extend(dates)

            # –ò—â–µ–º copyright
            copyright_pattern = r'¬©.*?(\d{4})|copyright.*?(\d{4})'
            copyright_matches = re.findall(copyright_pattern, text, re.IGNORECASE)
            for match in copyright_matches:
                year = match[0] or match[1]
                if year:
                    found_dates.append(year)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞—Ç—ã
            recent_dates = []
            for date_str in found_dates[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–∞—Ç
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥
                year_match = re.search(r'(\d{4})', date_str)
                if year_match:
                    year = int(year_match.group(1))
                    if year >= current_year - 1:
                        recent_dates.append(year)

            if recent_dates:
                self.results['–û–±–Ω–æ–≤–ª–µ–Ω–∏—è'] = f'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω –≤ {max(recent_dates)})'
            else:
                self.results['–û–±–Ω–æ–≤–ª–µ–Ω–∏—è'] = 'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (–Ω–µ—Ç —Å–≤–µ–∂–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π)'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {e}")
            self.results['–û–±–Ω–æ–≤–ª–µ–Ω–∏—è'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def check_page_structure(self, soup):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞"""
        try:
            # –ò—â–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            nav_elements = soup.find_all(['nav', 'ul', 'ol', 'menu'])

            # –°—á–∏—Ç–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            links = soup.find_all('a', href=True)
            internal_links = 0
            for link in links:
                href = link.get('href', '')
                if href.startswith(('#', '/')) or 'http' not in href:
                    internal_links += 1

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–æ—Ä–º (–ø—Ä–∏–∑–Ω–∞–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
            forms = soup.find_all('form')

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if len(nav_elements) < 1 and internal_links < 8 and len(forms) < 1:
                self.results['–°—Ç—Ä—É–∫—Ç—É—Ä–∞'] = 'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (–æ–¥–Ω–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π)'
            else:
                self.results['–°—Ç—Ä—É–∫—Ç—É—Ä–∞'] = f'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ ({internal_links} —Å—Å—ã–ª–æ–∫, {len(nav_elements)} –Ω–∞–≤–∏–≥–∞—Ü–∏–π)'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
            self.results['–°—Ç—Ä—É–∫—Ç—É—Ä–∞'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def check_builder(self, soup, url, response):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞"""
        try:
            domain = self.extract_domain(url).lower()
            page_text = str(soup).lower()
            html_text = response.text.lower()

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–æ–≤
            free_builders = {
                'Wix': ['wix', 'wixpress', 'wixsite.com'],
                'Weebly': ['weebly', 'weebly.com'],
                'WordPress.com': ['wordpress.com', 'wp.com', 'wp-content'],
                'Blogger': ['blogger', 'blogspot'],
                'Tilda': ['tilda', 'tilda.ws', 'tilda.cc'],
                'Ucoz': ['ucoz', 'ucoz.ru'],
                'Jimdo': ['jimdo', 'jimdosite'],
                'Webnode': ['webnode'],
            }

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ö–æ—Å—Ç–∏–Ω–≥–æ–≤
            free_hosting = [
                'github.io', 'netlify.app', 'vercel.app',
                'herokuapp.com', '000webhostapp.com',
                'glitch.me', 'repl.co', 'firebaseapp.com',
                'surge.sh', 'web.app'
            ]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã
            for builder_name, keywords in free_builders.items():
                if any(keyword in domain for keyword in keywords) or \
                        any(keyword in html_text for keyword in keywords):
                    self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = f'üî¥ –ù–µ–≥–∞—Ç–∏–≤ ({builder_name})'
                    return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ö–æ—Å—Ç–∏–Ω–≥
            if any(host in domain for host in free_hosting):
                self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = 'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ö–æ—Å—Ç–∏–Ω–≥)'
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏
            meta_generator = soup.find('meta', {'name': 'generator'})
            if meta_generator and meta_generator.get('content'):
                content = meta_generator['content'].lower()
                for builder_name, keywords in free_builders.items():
                    if any(keyword in content for keyword in keywords):
                        self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = f'üî¥ –ù–µ–≥–∞—Ç–∏–≤ ({builder_name})'
                        return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º JavaScript —Ñ–∞–π–ª—ã
            scripts = soup.find_all('script', src=True)
            for script in scripts:
                src = script.get('src', '').lower()
                for builder_name, keywords in free_builders.items():
                    if any(keyword in src for keyword in keywords):
                        self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = f'üî¥ –ù–µ–≥–∞—Ç–∏–≤ ({builder_name})'
                        return

            self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = 'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞: {e}")
            self.results['–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def check_owner(self, domain):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞ –¥–æ–º–µ–Ω–∞"""
        try:
            w = whois.whois(domain)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            if w.org:
                self.results['–í–ª–∞–¥–µ–ª–µ—Ü'] = f'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {w.org[:50]})'
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–º–µ–Ω–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞
            if w.name:
                name = str(w.name)
                # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –§–ò–û
                ru_name_pattern = r'^[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(\s+[–ê-–Ø–Å][–∞-—è—ë]+)?$'
                # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∏–º–µ–Ω
                en_name_pattern = r'^[A-Z][a-z]+\s+[A-Z][a-z]+$'

                if re.match(ru_name_pattern, name) or re.match(en_name_pattern, name):
                    self.results['–í–ª–∞–¥–µ–ª–µ—Ü'] = f'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (—á–∞—Å—Ç–Ω–æ–µ –ª–∏—Ü–æ: {name[:30]})'
                else:
                    self.results['–í–ª–∞–¥–µ–ª–µ—Ü'] = f'üü° –ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ ({name[:30]})'
            else:
                self.results['–í–ª–∞–¥–µ–ª–µ—Ü'] = 'üü° –ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–ª–∞–¥–µ–ª—å—Ü–∞: {e}")
            self.results['–í–ª–∞–¥–µ–ª–µ—Ü'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def check_reviews(self, domain):
        """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–∑—ã–≤–æ–≤"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –Ω—É–∂–Ω–æ API)
            clean_domain = domain.replace('www.', '').split('/')[0]

            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤—ã–∑–æ–≤ API –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–∑—ã–≤–æ–≤
            # –ù–∞–ø—Ä–∏–º–µ—Ä: trustpilot, –Ø–Ω–¥–µ–∫—Å.–û—Ç–∑—ã–≤—ã –∏ —Ç.–¥.

            # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
            self.results['–û—Ç–∑—ã–≤—ã'] = 'üü° –¢—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏'
            # self.results['–û—Ç–∑—ã–≤—ã'] = 'üî¥ –ù–µ–≥–∞—Ç–∏–≤ (–Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤)'
            # self.results['–û—Ç–∑—ã–≤—ã'] = 'üü¢ –ù–µ –Ω–µ–≥–∞—Ç–∏–≤ (–µ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã)'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–∑—ã–≤–æ–≤: {e}")
            self.results['–û—Ç–∑—ã–≤—ã'] = 'üü° –ù–µ–≥–∞—Ç–∏–≤ (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)'

    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""
        negative_count = 0
        warning_count = 0

        for value in self.results.values():
            if 'üî¥' in value:
                negative_count += 1
            elif 'üü°' in value and '–ù–µ–≥–∞—Ç–∏–≤' in value:
                negative_count += 1
                warning_count += 1
            elif 'üü°' in value:
                warning_count += 1

        report = "üìä *–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞*\n\n"

        for key, value in self.results.items():
            report += f"‚Ä¢ *{key}*: {value}\n"

        report += "\n" + "=" * 40 + "\n\n"

        if negative_count >= 2:
            report += "‚ùå *–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:* –ù–ï –ü–†–û–í–û–î–ò–¢–¨ –û–ü–ï–†–ê–¶–ò–ò –ù–ê –î–ê–ù–ù–û–ú –°–ê–ô–¢–ï\n\n"
            report += f"*–ü—Ä–∏—á–∏–Ω–∞:* {negative_count} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"
            if warning_count > 0:
                report += f" –∏ {warning_count} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"
        else:
            report += "‚ö†Ô∏è *–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:* –ú–û–ñ–ù–û –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ –° –û–°–¢–û–†–û–ñ–ù–û–°–¢–¨–Æ\n\n"
            report += f"*–°—Ç–∞—Ç—É—Å:* {negative_count} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"
            if warning_count > 0:
                report += f", {warning_count} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"

        return report